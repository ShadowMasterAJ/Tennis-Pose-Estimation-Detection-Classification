{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the backbone layers of efficientnet_b3\n",
      "Number of trainable parameters: 33929150\n",
      "Keypoints: torch.Size([64, 54])\n",
      "BBoxes: torch.Size([64, 4])\n",
      "Classification Logits: torch.Size([64, 4])\n",
      "Sample output:\n",
      "tensor([ 0.0262, -0.0847, -0.0710, -0.0327,  0.0069, -0.0109, -0.0463,  0.0475,\n",
      "         0.0798,  0.0187,  0.0610,  0.0315,  0.0295,  0.0237,  0.0387,  0.0365,\n",
      "         0.0329,  0.0595, -0.0216, -0.0419, -0.0883, -0.0175, -0.0206, -0.0083,\n",
      "         0.0781,  0.0149,  0.0383,  0.0636,  0.0687,  0.0527, -0.0635,  0.0590,\n",
      "         0.0019,  0.0253, -0.0326,  0.0665, -0.0646, -0.0321, -0.0357,  0.0751,\n",
      "         0.0684, -0.0320, -0.0116, -0.0186, -0.0331,  0.0104, -0.0413, -0.0328,\n",
      "        -0.0556,  0.0409,  0.0345, -0.0457,  0.0059, -0.0197], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.0427, 0.0727, 0.0006, 0.0018], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.2549, 0.2535, 0.2494, 0.2422], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# Add the project to the path for importing custom modules\n",
    "sys.path.append('C:\\\\Users\\\\arnav\\\\Documents\\\\University\\\\CS 5100 Foundations of Artificial Intelligence\\\\Final Project\\\\Final Project')\n",
    "\n",
    "from training.config import Config\n",
    "\n",
    "def get_backbone(name):\n",
    "    if name == 'efficientnet_b0':\n",
    "        return models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'efficientnet_b3':\n",
    "        return models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'resnet50':\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        return nn.Sequential(*list(backbone.children())[:-2])\n",
    "    elif name == 'efficientnet_b7':\n",
    "        return models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'resnet101':\n",
    "        backbone = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "        return nn.Sequential(*list(backbone.children())[:-2])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {name}\")\n",
    "\n",
    "class SPPLayer(nn.Module):\n",
    "    def __init__(self, num_levels):\n",
    "        super(SPPLayer, self).__init__()\n",
    "        self.num_levels = num_levels\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.size()\n",
    "        pooling_layers = []\n",
    "        for level in self.num_levels:\n",
    "            pooling = nn.AdaptiveMaxPool2d(output_size=(level, level))\n",
    "            pooling_layers.append(pooling(x).view(batch_size, -1))\n",
    "        return torch.cat(pooling_layers, dim=1)\n",
    "\n",
    "class TennisPoseSPP(nn.Module):\n",
    "    def __init__(self,backbone_name='efficientnet_b3'):\n",
    "        super(TennisPoseSPP, self).__init__()\n",
    "        self.num_keypoints = Config.NUM_KEYPOINTS\n",
    "        self.num_classes = Config.NUM_CLASSES\n",
    "        backbone_config = Config.get_backbone_layers(backbone_name)\n",
    "        if backbone_config is None:\n",
    "            raise ValueError(f\"Unknown backbone model: {backbone_name}\")\n",
    "\n",
    "        self.backbone = get_backbone(backbone_name)\n",
    "        self.backbone_channels = backbone_config['output_channels']\n",
    "\n",
    "        # Optionally freeze the backbone\n",
    "        if backbone_config.get('freeze_layers', False):\n",
    "            print(f\"Freezing the backbone layers of {backbone_name}\")\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Spatial Pyramid Pooling\n",
    "        self.spp = SPPLayer([1, 2, 4])\n",
    "\n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.backbone_channels * 21,  # Adjusted for SPP output\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=4)\n",
    "\n",
    "        # Keypoint Prediction Head\n",
    "        self.keypoint_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(Config.DROPOUT_RATE),\n",
    "            nn.Linear(128, self.num_keypoints * 3)\n",
    "        )\n",
    "\n",
    "        # Bounding Box Head\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(Config.DROPOUT_RATE),\n",
    "            nn.Linear(128, 4)  # x, y, width, height\n",
    "        )\n",
    "\n",
    "        # Classification Head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(Config.DROPOUT_RATE),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        spp_features = self.spp(features)\n",
    "        spp_features = spp_features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        lstm_out, _ = self.bilstm(spp_features)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Use the last output for prediction\n",
    "        final_features = attn_output[:, -1, :]\n",
    "\n",
    "        keypoints = self.keypoint_head(final_features)\n",
    "        bboxes = self.bbox_head(final_features)\n",
    "        classification_logits = self.classification_head(final_features)\n",
    "\n",
    "        return keypoints, bboxes, classification_logits\n",
    "\n",
    "class TennisPoseEstimationModel(nn.Module):\n",
    "    def __init__(self, num_keypoints=18, num_classes=4, backbone_name='efficientnet_b0'):\n",
    "        super(TennisPoseEstimationModel, self).__init__()\n",
    "        self.num_keypoints = num_keypoints\n",
    "        backbone_config = Config.get_backbone_layers(backbone_name)\n",
    "        if backbone_config is None:\n",
    "            raise ValueError(f\"Unknown backbone model: {backbone_name}\")\n",
    "\n",
    "        self.backbone = get_backbone(backbone_name)\n",
    "        self.backbone_channels = backbone_config['output_channels']\n",
    "\n",
    "        # Optionally freeze the backbone\n",
    "        if backbone_config.get('freeze_layers', False):\n",
    "            print(f\"Freezing the backbone layers of {backbone_name}\")\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.backbone_channels,\n",
    "            hidden_size=32,\n",
    "            num_layers=4,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=8)\n",
    "\n",
    "        # Keypoint Prediction Head\n",
    "        self.keypoint_head = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_keypoints * 3)\n",
    "        )\n",
    "\n",
    "        # Bounding Box Head\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # x, y, width, height\n",
    "        )\n",
    "\n",
    "        # Classification Head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        features = features.mean(dim=[2, 3]).view(batch_size, seq_len, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Use the last output for prediction\n",
    "        final_features = attn_output[:, -1, :]\n",
    "\n",
    "        keypoints = self.keypoint_head(final_features)\n",
    "        bboxes = self.bbox_head(final_features)\n",
    "        classification_logits = self.classification_head(final_features)\n",
    "\n",
    "        return keypoints, bboxes, classification_logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    device = Config.get_device()\n",
    "    model = TennisPoseSPP().to(device)\n",
    "    # print number of trainable parameters in the model\n",
    "    print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    input_tensor = torch.randn(64, 5, 3, 320, 320).to(device)  # Batch size 64, sequence length 5\n",
    "    keypoints, bboxes, classification_logits = model(input_tensor)\n",
    "\n",
    "    print(f\"Keypoints: {keypoints.shape}\")  # Expected: torch.Size([64, 54])\n",
    "    print(f\"BBoxes: {bboxes.shape}\")  # Expected: torch.Size([64, 4])\n",
    "    print(f\"Classification Logits: {classification_logits.shape}\")  # Expected: torch.Size([64, 4])\n",
    "    \n",
    "    print('Sample output:')\n",
    "    print(keypoints[0])\n",
    "    print(bboxes[0])\n",
    "    probabilities = torch.softmax(classification_logits, dim=1)\n",
    "    print(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
