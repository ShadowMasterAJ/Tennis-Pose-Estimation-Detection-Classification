{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add the project to the path for importing custom modules\n",
    "sys.path.append('C:\\\\Users\\\\arnav\\\\Documents\\\\University\\\\CS 5100 Foundations of Artificial Intelligence\\\\Final Project\\\\Final Project')\n",
    "\n",
    "from training.config import Config\n",
    "\n",
    "def get_backbone(name):\n",
    "    if name == 'efficientnet_b0':\n",
    "        return models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'efficientnet_b3':\n",
    "        return models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'resnet50':\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        return nn.Sequential(*list(backbone.children())[:-2])\n",
    "    elif name == 'efficientnet_b7':\n",
    "        return models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.IMAGENET1K_V1).features\n",
    "    elif name == 'resnet101':\n",
    "        backbone = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "        return nn.Sequential(*list(backbone.children())[:-2])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {name}\")\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention = torch.sigmoid(self.conv(x))\n",
    "        return x * attention\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class TennisNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=18, num_classes=4, backbone_name='efficientnet_b3'):\n",
    "        super(TennisNet, self).__init__()\n",
    "        \n",
    "        # Backbone\n",
    "        backbone_config = Config.get_backbone_layers(backbone_name)\n",
    "        if backbone_config is None:\n",
    "            raise ValueError(f\"Unknown backbone model: {backbone_name}\")\n",
    "\n",
    "        self.backbone = get_backbone(backbone_name)\n",
    "        self.backbone_channels = backbone_config['output_channels']\n",
    "\n",
    "        # Optionally freeze the backbone\n",
    "        if backbone_config.get('freeze_layers', False):\n",
    "            print(f\"Freezing the backbone layers of {backbone_name}\")\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Spatial Attention\n",
    "        self.spatial_attention = SpatialAttention(self.backbone_channels)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        self.conv3d = nn.Conv3d(self.backbone_channels, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.lstm = nn.LSTM(256, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Temporal Attention\n",
    "        self.temporal_attention = TemporalAttention(256)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.keypoint_head = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_keypoints * 3)\n",
    "        )\n",
    "        \n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 4)\n",
    "        )\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # print(f\"Input shape: {x.shape}\")  # Debugging: Print input shape\n",
    "            batch_size, seq_len, c, h, w = x.size()\n",
    "            \n",
    "            # Reshape for the backbone (merge batch and sequence dimensions)\n",
    "            x = x.view(batch_size * seq_len, c, h, w)\n",
    "            # print(f\"Reshaped input for backbone: {x.shape}\")  # Debugging: Print reshaped input shape\n",
    "            \n",
    "            # Extract spatial features using the backbone\n",
    "            features = self.backbone(x)  # (batch_size * seq_len, backbone_channels, H', W')\n",
    "            # print(f\"Features shape after backbone: {features.shape}\")  # Debugging: Print features shape after backbone\n",
    "            \n",
    "            # Apply spatial attention\n",
    "            features = self.spatial_attention(features)  # (batch_size * seq_len, backbone_channels, H', W')\n",
    "            # print(f\"Features shape after spatial attention: {features.shape}\")  # Debugging: Print features shape after spatial attention\n",
    "            \n",
    "            # Global average pooling on spatial dimensions\n",
    "            features = features.mean(dim=[2, 3])  # (batch_size * seq_len, backbone_channels)\n",
    "            # print(f\"Features shape after global pooling: {features.shape}\")  # Debugging: Print features shape after global pooling\n",
    "            \n",
    "            # Reshape back to sequence form\n",
    "            features = features.view(batch_size, seq_len, -1)  # (batch_size, seq_len, backbone_channels)\n",
    "            # print(f\"Features shape after reshaping to sequence: {features.shape}\")  # Debugging: Print reshaped features\n",
    "            \n",
    "            # Temporal modeling using 3D convolution\n",
    "            features = features.permute(0, 2, 1).unsqueeze(-1).unsqueeze(-1)  # (batch_size, backbone_channels, seq_len, 1, 1)\n",
    "            features = self.conv3d(features)  # (batch_size, 256, seq_len, 1, 1)\n",
    "            features = features.squeeze(-1).squeeze(-1).permute(0, 2, 1)  # (batch_size, seq_len, 256)\n",
    "            # print(f\"Features shape after 3D convolution: {features.shape}\")  # Debugging: Print features shape after 3D convolution\n",
    "            \n",
    "            # Temporal modeling using LSTM\n",
    "            lstm_out, _ = self.lstm(features)  # (batch_size, seq_len, 256)\n",
    "            # print(f\"LSTM output shape: {lstm_out.shape}\")  # Debugging: Print LSTM output shape\n",
    "            \n",
    "            # Temporal attention\n",
    "            attn_out = self.temporal_attention(lstm_out)  # (batch_size, seq_len, 256)\n",
    "            # print(f\"Attention output shape: {attn_out.shape}\")  # Debugging: Print attention output shape\n",
    "            \n",
    "            # Use the last output in the sequence for predictions\n",
    "            final_features = attn_out[:, -1, :]  # (batch_size, 256)\n",
    "            # print(f\"Final features shape: {final_features.shape}\")  # Debugging: Print final features shape\n",
    "            \n",
    "            # Predictions\n",
    "            keypoints = self.keypoint_head(final_features)  # (batch_size, num_keypoints * 3)\n",
    "            bboxes = self.bbox_head(final_features)  # (batch_size, 4)\n",
    "            classification_logits = self.classification_head(final_features)  # (batch_size, num_classes)\n",
    "            \n",
    "            return keypoints, bboxes, classification_logits\n",
    "        except RuntimeError as e:\n",
    "            # print(f\"RuntimeError: {e}\")\n",
    "            # print(f\"Shape mismatch at some layer. Input shape: {x.shape}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the backbone layers of efficientnet_b3\n",
      "Number of trainable parameters: 12227391\n",
      "Keypoints: torch.Size([32, 54])\n",
      "BBoxes: torch.Size([32, 4])\n",
      "Classification Logits: torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = Config.get_device()\n",
    "model = TennisNet().to(device)\n",
    "# print number of trainable parameters in the model\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "input_tensor = torch.randn(32, 5, 3, 320, 320).to(device)  # Batch size 32, sequence length 16\n",
    "keypoints, bboxes, classification_logits = model(input_tensor)\n",
    "\n",
    "print(f\"Keypoints: {keypoints.shape}\")  # Expected: torch.Size([32, 54])\n",
    "print(f\"BBoxes: {bboxes.shape}\")  # Expected: torch.Size([32, 4])\n",
    "print(f\"Classification Logits: {classification_logits.shape}\")  # Expected: torch.Size([32, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
