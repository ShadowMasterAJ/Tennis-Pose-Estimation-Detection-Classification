{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def draw_bounding_box(img, bbox, color=(0, 255, 0), thickness=2):\n",
    "    x, y, w, h = [int(coord) for coord in bbox]\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness)\n",
    "\n",
    "def draw_keypoints_and_skeleton(img, keypoints):\n",
    "    \"\"\"\n",
    "    Draws keypoints and skeleton on an image.\n",
    "    Parameters:\n",
    "    img (numpy.ndarray): The image on which to draw the keypoints and skeleton.\n",
    "    keypoints (numpy.ndarray): An array of shape (N, 3) where N is the number of keypoints.\n",
    "                               Each keypoint is represented by (x, y, v) where (x, y) are the coordinates\n",
    "                               and v is the visibility flag (0: not visible, 1: visible).\n",
    "    Returns:\n",
    "    None: The function modifies the input image in place by drawing the keypoints and skeleton.\n",
    "    \"\"\"\n",
    "    \n",
    "    joint_colors = [\n",
    "            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255),\n",
    "            (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n",
    "            (128, 0, 128), (0, 128, 128), (64, 0, 0), (0, 64, 0), (0, 0, 64),\n",
    "            (64, 64, 0), (64, 0, 64), (0, 64, 64)\n",
    "        ]\n",
    "    skeleton = [(1, 2), (1, 3), (1, 18), (2, 4), (3, 5), (6, 8), (6, 12), (6, 18), (7, 9), (7, 13), (7, 18), (8, 10), (9, 11), (12, 14), (12, 13), (13, 15), (14, 16), (15, 17)]\n",
    "\n",
    "    for j, (x, y, v) in enumerate(keypoints):\n",
    "        if v > 0:\n",
    "            cv2.circle(img, (int(x), int(y)), 3, joint_colors[j], -1)\n",
    "            cv2.putText(img, str(j+1), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    for i, (start_point, end_point) in enumerate(skeleton):\n",
    "        if keypoints[start_point-1, 2] > 0 and keypoints[end_point-1, 2] > 0:\n",
    "            start = tuple(keypoints[start_point-1, :2].astype(int))\n",
    "            end = tuple(keypoints[end_point-1, :2].astype(int))\n",
    "            cv2.line(img, start, end, joint_colors[i % len(joint_colors)], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_image(img_info, annotation, base_path, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Processes an image by reading it from the specified path, converting its color space,\n",
    "    and optionally drawing bounding boxes and keypoints based on the provided annotation.\n",
    "    Args:\n",
    "        img_info (dict): Dictionary containing image metadata, including 'path', 'width', and 'height'.\n",
    "        annotation (dict): Dictionary containing annotation data, including 'bbox' and 'keypoints'.\n",
    "                           If None, no annotations will be drawn.\n",
    "        base_path (str): Base path to the directory containing the image.\n",
    "        img_width (int): The width to which the image should be scaled.\n",
    "        img_height (int): The height to which the image should be scaled.\n",
    "    Returns:\n",
    "        numpy.ndarray: The processed image with optional annotations drawn.\n",
    "    \"\"\"\n",
    "    \n",
    "    img_path = f\"{base_path}/{img_info['path'].lstrip('../')}\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if annotation:\n",
    "        bbox = [int(coord * img_width / img_info['width']) for coord in annotation['bbox']]\n",
    "        draw_bounding_box(img, bbox)\n",
    "        \n",
    "        keypoints = np.array(annotation['keypoints']).reshape(-1, 3)\n",
    "        keypoints[:, 0] = keypoints[:, 0] * img_width / img_info['width']\n",
    "        keypoints[:, 1] = keypoints[:, 1] * img_height / img_info['height']\n",
    "        \n",
    "        draw_keypoints_and_skeleton(img, keypoints)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualise_dataset(json_file, base_path, num_images=2):\n",
    "    \"\"\"\n",
    "    Visualises a subset of images from a dataset described in a JSON file.\n",
    "    Parameters:\n",
    "    json_file (str): Path to the JSON file containing dataset information.\n",
    "    base_path (str): Base path to the directory containing the images.\n",
    "    num_images (int, optional): Number of images to visualise. Defaults to 2.\n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: A matplotlib figure object containing the visualised images.\n",
    "    The JSON file should contain:\n",
    "    - 'images': A list of dictionaries, each with keys 'id', 'file_name', 'width', and 'height'.\n",
    "    - 'annotations': A list of dictionaries, each with keys 'image_id' and other annotation details.\n",
    "    The function randomly selects `num_images` from the dataset, processes them, and displays them in a matplotlib figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    selected_images = random.sample(data['images'], num_images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 6))\n",
    "    \n",
    "    for i, (ax, img_info) in enumerate(zip(axes, selected_images)):\n",
    "        annotation = next((ann for ann in data['annotations'] if ann['image_id'] == img_info['id']), None)\n",
    "        img = process_image(img_info, annotation, base_path, img_info['width'], img_info['height'])\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Image {i+1}: {img_info['file_name']}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualise_dataloader_sample(ax, image, bbox, keypoints, label, title):\n",
    "    \"\"\"\n",
    "    Visualize a sample from the dataloader by displaying the image with bounding boxes and keypoints.\n",
    "    Parameters:\n",
    "        ax (matplotlib.axes.Axes): The axes on which to plot the image.\n",
    "        image (torch.Tensor): The image tensor to be visualized.\n",
    "        bbox (list or torch.Tensor): The bounding box coordinates in the format [x_min, y_min, x_max, y_max].\n",
    "        keypoints (torch.Tensor): The keypoints tensor with shape (num_keypoints, 3).\n",
    "        label (int): The label index corresponding to the action class.\n",
    "        title (str): The title for the plot.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert tensor image to numpy array and denormalize\n",
    "    img_np = image.cpu().permute(1, 2, 0).detach().numpy()  # Ensure it's on CPU and detached\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Denormalize the image\n",
    "    img_np = img_np * std + mean\n",
    "    img_np = np.clip(img_np, 0, 1)  # Clip values to ensure they are within [0, 1]\n",
    "    \n",
    "    # Convert to uint8 and BGR format for OpenCV\n",
    "    img_np = (img_np * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    img_rgb = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = 1,1 #img_rgb.shape[:2]\n",
    "    \n",
    "    # Draw bounding box (converting from normalized to pixel space)\n",
    "    # Expecting bbox in format [x_min, y_min, x_max, y_max]\n",
    "    x_min = int(bbox[0] * width)\n",
    "    y_min = int(bbox[1] * height)\n",
    "    x_max = int(bbox[2] * width)\n",
    "    y_max = int(bbox[3] * height)\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(img_rgb, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)  # Green box\n",
    "\n",
    "    # Draw keypoints\n",
    "    keypoints_pixel = keypoints.cpu().detach().view(-1, 3).numpy()  # Ensure it's on CPU and detached\n",
    "    keypoints_pixel[:, 0] *= width  # Scale x to image width\n",
    "    keypoints_pixel[:, 1] *= height  # Scale y to image height\n",
    "    \n",
    "    # Assuming draw_keypoints_and_skeleton is defined and works properly\n",
    "    draw_keypoints_and_skeleton(img_rgb, keypoints_pixel)  \n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"{title}: {['backhand', 'forehand', 'serve', 'ready_position'][label]}\")\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_results(all_images, all_labels, all_predictions, all_bboxes, all_keypoints, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize the results of image predictions by displaying the correct and predicted labels side by side.\n",
    "    Parameters:\n",
    "    - all_images (list or tensor): List or tensor of images.\n",
    "    - all_labels (list or tensor): List or tensor of true labels.\n",
    "    - all_predictions (list or tensor): List or tensor of predicted labels.\n",
    "    - all_bboxes (list or tensor): List or tensor of bounding boxes.\n",
    "    - all_keypoints (list or tensor): List or tensor of keypoints.\n",
    "    - num_samples (int, optional): Number of samples to visualize. Default is 3.\n",
    "    Returns:\n",
    "    - fig (matplotlib.figure.Figure): The matplotlib figure object containing the visualizations.\n",
    "    The function randomly selects a specified number of samples from the provided data and visualizes them.\n",
    "    Each sample is displayed in two columns: one for the correct label and one for the predicted label.\n",
    "    Bounding boxes and keypoints are also displayed on the images.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axs = plt.subplots(num_samples, 2, figsize=(12, 6*num_samples))\n",
    "    fig.suptitle('Correct vs Predicted Results', fontsize=16)\n",
    "\n",
    "    random_indices = random.sample(range(len(all_images)), num_samples)\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image = all_images[idx]\n",
    "        true_label = all_labels[idx]\n",
    "        pred_label = all_predictions[idx]\n",
    "        bbox = all_bboxes[idx]\n",
    "        keypoints = all_keypoints[idx].reshape(-1, 2)\n",
    "        \n",
    "        # Denormalize the image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = image * std + mean\n",
    "        image = image.clamp(0, 1)  # Ensure values are in [0, 1] range\n",
    "        \n",
    "        # Display correct label image\n",
    "        visualise_dataloader_sample(axs[i, 0], image, bbox, keypoints, true_label, 'Correct')\n",
    "        \n",
    "        # Display predicted label image\n",
    "        visualise_dataloader_sample(axs[i, 1], image, bbox, keypoints, pred_label, 'Predicted')\n",
    "\n",
    "        # Print bbox and keypoints values (in pixel coordinates)\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        height, width = 224, 224  # Assuming resized images\n",
    "        bbox_np = bbox.cpu().numpy()\n",
    "        keypoints_np = keypoints.cpu().numpy()\n",
    "        \n",
    "        print(\"Bounding Box:\")\n",
    "        print(f\"{'':>10}{'Correct':>15}{'Predicted':>15}\")\n",
    "        print(f\"{'x:':<10}{bbox_np[0]*width:15.2f}{bbox_np[0]*width:15.2f}\")\n",
    "        print(f\"{'y:':<10}{bbox_np[1]*height:15.2f}{bbox_np[1]*height:15.2f}\")\n",
    "        print(f\"{'width:':<10}{bbox_np[2]*width:15.2f}{bbox_np[2]*width:15.2f}\")\n",
    "        print(f\"{'height:':<10}{bbox_np[3]*height:15.2f}{bbox_np[3]*height:15.2f}\")\n",
    "        \n",
    "        print(\"\\nKeypoints:\")\n",
    "        print(f\"{'Point':>5}{'Correct X':>15}{'Correct Y':>15}{'Predicted X':>15}{'Predicted Y':>15}\")\n",
    "        for j, (kx, ky) in enumerate(keypoints_np):\n",
    "            print(f\"{j+1:5d}{kx*width:15.2f}{ky*height:15.2f}{kx*width:15.2f}{ky*height:15.2f}\")\n",
    "        print()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video(model, input_video_path, output_video_path, device, preprocess_image):\n",
    "    \"\"\"\n",
    "    Processes an input video frame by frame, performs inference using a given model, and saves the annotated video.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model used for inference.\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_path (str): Path to save the output annotated video file.\n",
    "        device (torch.device): The device (CPU or GPU) to run the model on.\n",
    "        preprocess_image (function): A function to preprocess each frame before inference.\n",
    "    Returns:\n",
    "        None\n",
    "    The function performs the following steps:\n",
    "    1. Opens the input video file.\n",
    "    2. Retrieves video properties such as width, height, frames per second (fps), and total frames.\n",
    "    3. Creates a VideoWriter object to save the output video.\n",
    "    4. Processes each frame of the video:\n",
    "        - Converts the frame to a PIL Image.\n",
    "        - Preprocesses the image and performs inference using the model.\n",
    "        - Extracts keypoints, bounding box, and classification results from the model's output.\n",
    "        - Draws the bounding box and keypoints on the frame.\n",
    "        - Adds the predicted class label to the frame.\n",
    "        - Writes the annotated frame to the output video.\n",
    "    5. Releases video resources and saves the annotated video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    for _ in tqdm(range(min(100, total_frames)), desc=\"Processing video\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to PIL Image\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Perform inference\n",
    "        preprocessed_image = preprocess_image(pil_image)\n",
    "        with torch.no_grad():\n",
    "            pred_keypoints, pred_bbox, pred_classification = model(preprocessed_image)\n",
    "        \n",
    "        # Convert the predicted keypoints and bounding box to numpy arrays\n",
    "        keypoints = pred_keypoints.squeeze().cpu().numpy()\n",
    "        bbox = pred_bbox.squeeze().cpu().numpy()\n",
    "\n",
    "        # Convert the predicted class probabilities to numpy array and get the predicted class\n",
    "        class_probs = F.softmax(pred_classification, dim=1).squeeze().cpu().numpy()\n",
    "        predicted_class = np.argmax(class_probs)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(frame, (int(x*width), int(y*height)), (int((x+w)*width), int((y+h)*height)), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for kp in keypoints.reshape(-1, 2):\n",
    "            cv2.circle(frame, (int(kp[0]*width), int(kp[1]*height)), 3, (255, 0, 0), -1)\n",
    "        \n",
    "        # Add class label\n",
    "        class_name = f\"Class {predicted_class}\"\n",
    "        cv2.putText(frame, f\"Class: {class_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"Video processing complete. Annotated video saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
