{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TennisDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading and processing tennis-related data.\n",
    "    Args:\n",
    "        data (List[Dict]): A list of dictionaries where each dictionary contains information about an image and its annotations.\n",
    "        transform (callable, optional): A function/transform to apply to the images.\n",
    "        sequence_length (Optional[int], optional): If provided, indicates that the data should be treated as sequences of this length.\n",
    "    Attributes:\n",
    "        sequence_length (Optional[int]): The length of the sequences if provided.\n",
    "        transform (callable, optional): A function/transform to apply to the images.\n",
    "        data (List[Dict]): The dataset containing image paths and annotations.\n",
    "    Methods:\n",
    "        __len__() -> int:\n",
    "            Returns the number of items in the dataset.\n",
    "        __getitem__(idx: int):\n",
    "            Retrieves the item at the given index. If sequence_length is provided, retrieves a sequence of items.\n",
    "            Args:\n",
    "                idx (int): The index of the item to retrieve.\n",
    "            Returns:\n",
    "                tuple: A tuple containing the image(s), bounding box(es), keypoint(s), and label.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], transform=None, sequence_length: Optional[int] = None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.sequence_length:\n",
    "            # If sequence_length is provided, retrieve a sequence of items\n",
    "            sequence = self.data[idx]\n",
    "            frames, bboxes, keypoints = [], [], []\n",
    "            for item in sequence:\n",
    "                original_width, original_height = item['width'], item['height']\n",
    "\n",
    "            # Open and transform the image\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            new_height, new_width = image.shape[1], image.shape[2]\n",
    "\n",
    "            # Compute scaling factors\n",
    "            scale_x = new_width / original_width\n",
    "            scale_y = new_height / original_height\n",
    "\n",
    "            # Append transformed image, normalized bounding boxes, and keypoints\n",
    "            frames.append(image)\n",
    "            bboxes.append(normalize_bbox(item['bbox'], scale_x, scale_y))\n",
    "            keypoints.append(normalize_keypoints(item['keypoints'], scale_x, scale_y))\n",
    "            \n",
    "            # Stack frames, bounding boxes, and keypoints into tensors\n",
    "            frames = torch.stack(frames)\n",
    "            bboxes = torch.stack(bboxes)\n",
    "            keypoints = torch.stack(keypoints)\n",
    "            \n",
    "            # Convert label to tensor\n",
    "            label = torch.tensor(['backhand', 'forehand', 'serve', 'ready_position'].index(sequence[0]['label']))\n",
    "            return frames, bboxes, keypoints, label\n",
    "        \n",
    "        else:\n",
    "            # If sequence_length is not provided, retrieve a single item\n",
    "            item = self.data[idx]\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "            original_width, original_height = image.size\n",
    "\n",
    "            # Transform the image\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            new_height, new_width = image.shape[1], image.shape[2]\n",
    "\n",
    "            # Compute scaling factors\n",
    "            scale_x = new_width / original_width\n",
    "            scale_y = new_height / original_height\n",
    "\n",
    "            # Normalize bounding boxes and keypoints\n",
    "            bboxes = normalize_bbox(item['bbox'], scale_x, scale_y)\n",
    "            keypoints = normalize_keypoints(item['keypoints'], scale_x, scale_y)\n",
    "            \n",
    "            # Convert label to tensor\n",
    "            label = torch.tensor(['backhand', 'forehand', 'serve', 'ready_position'].index(item['label']))\n",
    "        \n",
    "            return image, bboxes, keypoints, label\n",
    "\n",
    "def get_train_transform() -> transforms.Compose:\n",
    "    \"\"\"\n",
    "    Returns a composition of image transformations to be applied to the training dataset.\n",
    "    The transformations include:\n",
    "    - Resizing the image to 320x320 pixels.\n",
    "    - Applying random changes in brightness, contrast, saturation, and hue.\n",
    "    - Applying Gaussian blur with a kernel size of 5x5 and a sigma range of 0.1 to 2.0.\n",
    "    - Converting the image to a tensor.\n",
    "    - Normalizing the image tensor with mean and standard deviation values.\n",
    "    Returns:\n",
    "        transforms.Compose: A composition of the specified image transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "def get_val_transform():\n",
    "    \"\"\"\n",
    "    Returns a composed transform for validation data preprocessing.\n",
    "    The transform includes the following steps:\n",
    "    1. Resize the image to 320x320 pixels.\n",
    "    2. Convert the image to a tensor.\n",
    "    3. Normalize the image tensor with mean and standard deviation values.\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: A composed transform for validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def normalize_bbox(bbox: List[float], scale_x: float, scale_y: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize a bounding box by scaling its coordinates.\n",
    "    Args:\n",
    "        bbox (List[float]): A list of four floats representing the bounding box in the format [x, y, width, height].\n",
    "        scale_x (float): The scaling factor for the x-axis.\n",
    "        scale_y (float): The scaling factor for the y-axis.\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized bounding box coordinates in the format [xmin, ymin, xmax, ymax].\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, width, height = bbox\n",
    "    xmin = max(0, x * scale_x)\n",
    "    ymin = max(0, y * scale_y)\n",
    "    xmax = max(0, (x + width) * scale_x)\n",
    "    ymax = max(0, (y + height) * scale_y)\n",
    "    normalized = [xmin, ymin, xmax, ymax]\n",
    "    return torch.tensor(normalized, dtype=torch.float32)\n",
    "\n",
    "def normalize_keypoints(keypoints: List[float], scale_x: float, scale_y: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize keypoints by scaling the x and y coordinates and keeping the visibility value unchanged.\n",
    "    Args:\n",
    "        keypoints (List[float]): A list of keypoints where each keypoint is represented by three values [x, y, v].\n",
    "        scale_x (float): The scaling factor for the x coordinates.\n",
    "        scale_y (float): The scaling factor for the y coordinates.\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized keypoints with the same structure as the input list.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized = []\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        x = max(0, keypoints[i] * scale_x)\n",
    "        y = max(0, keypoints[i + 1] * scale_y)\n",
    "        v = keypoints[i + 2]\n",
    "        normalized.extend([x, y, v])\n",
    "    return torch.tensor(normalized, dtype=torch.float32)\n",
    "\n",
    "def load_annotations(json_files: List[str], base_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load annotations from a list of JSON files and return a list of dictionaries containing image data and annotations.\n",
    "    Args:\n",
    "        json_files (List[str]): A list of JSON file names containing annotation data.\n",
    "        base_path (str): The base directory path where the JSON files and images are located.\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries, each containing the following keys:\n",
    "            - 'image_path' (str): The path to the image file.\n",
    "            - 'bbox' (List[float]): The bounding box coordinates of the annotation.\n",
    "            - 'keypoints' (List[float]): The keypoints of the annotation.\n",
    "            - 'label' (str): The label of the annotation (shot type).\n",
    "            - 'id' (int): The ID of the image.\n",
    "            - 'height' (int): The height of the image.\n",
    "            - 'width' (int): The width of the image.\n",
    "    Raises:\n",
    "        FileNotFoundError: If any of the JSON files are not found in the specified path.\n",
    "    Example:\n",
    "        json_files = ['annotation1.json', 'annotation2.json']\n",
    "        base_path = '/path/to/dataset'\n",
    "        annotations = load_annotations(json_files, base_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    for json_file in json_files:\n",
    "        json_file_path = os.path.join(base_path, 'annotations', json_file)\n",
    "        try:\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Annotation file {json_file_path} not found.\")\n",
    "            continue\n",
    "\n",
    "        shot_type = data['categories'][0]['name'].lower()\n",
    "        for img_info in data['images']:\n",
    "            img_path = os.path.join(base_path, img_info['path'].lstrip('../'))\n",
    "            annotation = next((ann for ann in data['annotations'] if ann['image_id'] == img_info['id']), None)\n",
    "            if annotation:\n",
    "                all_data.append({\n",
    "                    'image_path': img_path,\n",
    "                    'bbox': annotation['bbox'],\n",
    "                    'keypoints': annotation['keypoints'],\n",
    "                    'label': shot_type,\n",
    "                    'id': img_info['id'],\n",
    "                    'height': img_info['height'],\n",
    "                    'width': img_info['width']\n",
    "                })\n",
    "    return all_data\n",
    "\n",
    "def sequentialize_data(data: List[Dict], sequence_length: int) -> List[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Converts a list of dictionaries into a list of sequences of dictionaries, \n",
    "    where each sequence has a specified length.\n",
    "    Args:\n",
    "        data (List[Dict]): A list of dictionaries, each containing data entries.\n",
    "        sequence_length (int): The length of each sequence to be generated.\n",
    "    Returns:\n",
    "        List[List[Dict]]: A list of sequences, where each sequence is a list of dictionaries.\n",
    "    Example:\n",
    "        data = [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, {'id': 3, 'value': 'c'}]\n",
    "        sequence_length = 2\n",
    "        result = sequentialize_data(data, sequence_length)\n",
    "        # result will be:\n",
    "        # [\n",
    "        #     [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}],\n",
    "        #     [{'id': 2, 'value': 'b'}, {'id': 3, 'value': 'c'}]\n",
    "        # ]\n",
    "    \"\"\"\n",
    "    \n",
    "    sequences = []\n",
    "    data.sort(key=lambda x: x['id'])\n",
    "    for i in range(0, len(data), 500):\n",
    "        batch = data[i:i + 500]\n",
    "        for j in range(len(batch) - sequence_length + 1):\n",
    "            sequence = batch[j:j + sequence_length]\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "def split_data(data: List[Dict], sequence_length: int = 1) -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Splits the input data into training, validation, and test sets.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : List[Dict]\n",
    "        A list of dictionaries where each dictionary represents a data point.\n",
    "    sequence_length : int, optional\n",
    "        The length of the sequence to be considered for splitting. Default is 1.\n",
    "        If greater than 1, the data is split sequentially.\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[List[Dict], List[Dict], List[Dict]]\n",
    "        A tuple containing three lists of dictionaries: train_data, val_data, and test_data.\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If there is any overlap between the train, validation, and test sets.\n",
    "    Notes:\n",
    "    ------\n",
    "    - When sequence_length is greater than 1, the data is split sequentially into categories,\n",
    "      and a round-robin selection method is used to ensure balanced splits.\n",
    "    - The function ensures that there are no overlaps between the train, validation, and test sets.\n",
    "    - If sequence_length is 1, the data is shuffled and split randomly using a 70-15-15 ratio for train, validation, and test sets respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    if sequence_length > 1:\n",
    "        # Split data sequentially\n",
    "        \n",
    "        train_ratio = 0.7\n",
    "        val_ratio = 0.15\n",
    "\n",
    "        total_frames = len(data)\n",
    "        total_categories = 4\n",
    "\n",
    "        # Calculate sizes\n",
    "        train_size = int(train_ratio * total_frames)\n",
    "        val_size = int(val_ratio * total_frames)\n",
    "\n",
    "        # Initialize splits\n",
    "        train_data = []\n",
    "        val_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        category_data = [[] for _ in range(total_categories)]\n",
    "        for i, ls in enumerate(category_data):\n",
    "            ls.append(data[i*(len(data)//total_categories):(i+1)*(len(data)//total_categories)])\n",
    "        \n",
    "        category_data = [item for sublist in category_data for item in sublist]\n",
    "        \n",
    "        # Function to round-robin select frames\n",
    "        def round_robin_select(target_size, start_indices):\n",
    "            selected_data = []\n",
    "            indices = start_indices.copy()\n",
    "            \n",
    "            while len(selected_data) < target_size:\n",
    "                for i in range(total_categories):\n",
    "                    if indices[i] < len(category_data[i]):\n",
    "                        selected_data.append(category_data[i][indices[i]])\n",
    "                        indices[i] += 1\n",
    "                    if len(selected_data) == target_size:\n",
    "                        break\n",
    "                        \n",
    "                if min(indices) >= len(category_data[0]):\n",
    "                    break\n",
    "              \n",
    "            for i in range(len(indices)):\n",
    "                indices[i] += sequence_length\n",
    "                indices[i] -= 1\n",
    "            return selected_data, indices\n",
    "\n",
    "        # Initialize indices for each category\n",
    "        start_indices = [0] * total_categories\n",
    "\n",
    "        # Fill the train, val, and test sets\n",
    "        train_data, start_indices = round_robin_select(train_size, start_indices)\n",
    "        val_data, start_indices = round_robin_select(val_size, start_indices)\n",
    "        remaining_size = total_frames - len(train_data) - len(val_data)\n",
    "        test_data, start_indices = round_robin_select(remaining_size, start_indices)\n",
    " \n",
    "        # Ensure no overlaps\n",
    "        train_ids = {item['id'] for sequence in train_data for item in sequence}\n",
    "        val_ids = {item['id'] for sequence in val_data for item in sequence}\n",
    "        test_ids = {item['id'] for sequence in test_data for item in sequence}\n",
    "\n",
    "        train_val_overlap = train_ids.intersection(val_ids)\n",
    "        train_test_overlap = train_ids.intersection(test_ids)\n",
    "        val_test_overlap = val_ids.intersection(test_ids)\n",
    "\n",
    "        if train_val_overlap:\n",
    "            raise ValueError(f\"Overlap between train and val: {train_val_overlap}\")\n",
    "        if train_test_overlap:\n",
    "            raise ValueError(f\"Overlap between train and test: {train_test_overlap}\")\n",
    "        if val_test_overlap:\n",
    "            raise ValueError(f\"Overlap between val and test: {val_test_overlap}\")\n",
    "        else:\n",
    "            print('No overlaps found in data')\n",
    "    else:\n",
    "        random.shuffle(data)\n",
    "        train_data, val_test_data = train_test_split(data, train_size=0.7, random_state=42)\n",
    "        val_data, test_data = train_test_split(val_test_data, train_size=0.5, random_state=42)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def get_datasets(json_files: List[str], base_path: str, sequence_length: Optional[int] = None) -> Tuple[TennisDataset, TennisDataset, TennisDataset]:\n",
    "    \"\"\"\n",
    "    Load datasets from JSON annotation files, apply transformations, and split into training, validation, and test sets.\n",
    "    Args:\n",
    "        json_files (List[str]): List of paths to JSON annotation files.\n",
    "        base_path (str): Base path to the dataset directory.\n",
    "        sequence_length (Optional[int], optional): Length of sequences for sequential data. Defaults to None.\n",
    "    Returns:\n",
    "        Tuple[TennisDataset, TennisDataset, TennisDataset]: A tuple containing the training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = load_annotations(json_files, base_path)\n",
    "    transform_train = get_train_transform()\n",
    "    transform_val = get_val_transform()\n",
    "    \n",
    "    if sequence_length:\n",
    "        # Sequentialize first\n",
    "        sequences = sequentialize_data(all_data, sequence_length)\n",
    "        train_data, val_data, test_data = split_data(sequences, sequence_length)\n",
    "    else:\n",
    "        # Split first\n",
    "        train_data, val_data, test_data = split_data(all_data)\n",
    "    train_dataset = TennisDataset(train_data, transform=transform_train, sequence_length=sequence_length)\n",
    "    val_dataset = TennisDataset(val_data,transform=transform_val, sequence_length=sequence_length)\n",
    "    test_dataset = TennisDataset(test_data,transform=transform_val, sequence_length=sequence_length)\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-sequential - Train: 1400, Val: 300, Test: 300\n",
      "No overlaps found in data\n",
      "Sequential - Train: 1388, Val: 297, Test: 267\n",
      "Train Dataset - Sample:\n",
      "Image shape: torch.Size([3, 320, 320])\n",
      "BBoxes: tensor([  0.2500, 108.4444,  32.7500, 205.3333])\n",
      "Keypoints: tensor([ 19.5000, 120.0000,   1.0000,  17.5000, 118.6667,   1.0000,  18.7500,\n",
      "        118.2222,   1.0000,  14.5000, 120.8889,   1.0000,  17.5000, 121.3333,\n",
      "          2.0000,  12.7500, 133.7778,   2.0000,  22.2500, 125.7778,   2.0000,\n",
      "         16.2500, 149.3333,   2.0000,  17.5000, 128.0000,   1.0000,  21.2500,\n",
      "        134.6667,   1.0000,  11.2500, 131.1111,   2.0000,  19.2500, 159.5556,\n",
      "          2.0000,  27.5000, 155.1111,   2.0000,  21.0000, 165.7778,   2.0000,\n",
      "         29.0000, 173.7778,   2.0000,  19.7500, 183.1111,   2.0000,  24.5000,\n",
      "        188.4444,   2.0000,  16.5000, 124.4444,   2.0000])\n",
      "Label: 1\n",
      "Validation Dataset - Sample:\n",
      "Image shape: torch.Size([3, 320, 320])\n",
      "BBoxes: tensor([ 85.0000, 141.3333, 119.0000, 262.2222])\n",
      "Keypoints: tensor([ 97.7500, 155.5556,   1.0000,  95.7500, 151.1111,   1.0000,  97.5000,\n",
      "        150.2222,   1.0000,  94.7500, 156.0000,   2.0000,  99.2500, 153.3333,\n",
      "          2.0000,  92.7500, 163.5556,   2.0000, 104.5000, 167.5556,   2.0000,\n",
      "         96.5000, 164.8889,   1.0000, 107.2500, 181.7778,   2.0000, 106.2500,\n",
      "        164.0000,   2.0000, 109.0000, 165.7778,   2.0000,  90.2500, 199.5556,\n",
      "          2.0000, 102.2500, 199.5556,   2.0000,  90.0000, 222.2222,   2.0000,\n",
      "         98.5000, 222.6667,   2.0000,  93.0000, 248.8889,   2.0000,  99.0000,\n",
      "        248.0000,   2.0000,  97.7500, 159.1111,   2.0000])\n",
      "Label: 0\n",
      "Test Dataset - Sample:\n",
      "Image shape: torch.Size([3, 320, 320])\n",
      "BBoxes: tensor([ 88.7500, 137.3333, 119.7500, 256.4445])\n",
      "Keypoints: tensor([107.0000, 149.7778,   1.0000, 105.7500, 148.0000,   1.0000, 108.2500,\n",
      "        148.0000,   1.0000, 105.2500, 151.1111,   2.0000, 110.7500, 151.1111,\n",
      "          2.0000, 102.0000, 159.5556,   2.0000, 114.5000, 166.2222,   2.0000,\n",
      "        101.7500, 177.3333,   1.0000, 114.7500, 187.5556,   2.0000, 106.2500,\n",
      "        172.0000,   1.0000, 114.2500, 190.2222,   1.0000, 100.5000, 195.1111,\n",
      "          2.0000, 110.7500, 194.2222,   2.0000,  97.7500, 219.1111,   2.0000,\n",
      "        113.7500, 219.1111,   2.0000,  93.7500, 241.3333,   2.0000, 114.5000,\n",
      "        244.8889,   2.0000, 107.2500, 155.5556,   2.0000])\n",
      "Label: 3\n",
      "\n",
      "\n",
      "Train Dataset Sequential - Sequence Sample:\n",
      "Frames shape: torch.Size([5, 3, 320, 320])\n",
      "BBoxes shape: torch.Size([5, 4])\n",
      "Keypoints shape: torch.Size([5, 54])\n",
      "Label: 0\n",
      "Validation Dataset Sequential - Sequence Sample:\n",
      "Frames shape: torch.Size([5, 3, 320, 320])\n",
      "BBoxes shape: torch.Size([5, 4])\n",
      "Keypoints shape: torch.Size([5, 54])\n",
      "Label: 0\n",
      "Test Dataset Sequential - Sequence Sample:\n",
      "Frames shape: torch.Size([5, 3, 320, 320])\n",
      "BBoxes shape: torch.Size([5, 4])\n",
      "Keypoints shape: torch.Size([5, 54])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "json_files = ['backhand.json', 'forehand.json', 'serve.json', 'ready_position.json']\n",
    "base_path = \"../og_dataset\"\n",
    "# Non-sequential data\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(json_files, base_path)\n",
    "print(f\"Non-sequential - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "train_dataset_seq, val_dataset_seq, test_dataset_seq = get_datasets(json_files, base_path, sequence_length=5)\n",
    "print(f\"Sequential - Train: {len(train_dataset_seq)}, Val: {len(val_dataset_seq)}, Test: {len(test_dataset_seq)}\")\n",
    "# Print a sample of the data for each dataset\n",
    "def print_sample(dataset, name):\n",
    "    sample = dataset[0]\n",
    "    if dataset.sequence_length:\n",
    "        frames, bboxes, keypoints, label = sample\n",
    "        print(f\"{name} - Sequence Sample:\")\n",
    "        print(f\"Frames shape: {frames.shape}\")\n",
    "        print(f\"BBoxes shape: {bboxes.shape}\")\n",
    "        print(f\"Keypoints shape: {keypoints.shape}\")\n",
    "        print(f\"Label: {label}\")\n",
    "    else:\n",
    "        image, bboxes, keypoints, label = sample\n",
    "        print(f\"{name} - Sample:\")\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "        print(f\"BBoxes: {bboxes}\")\n",
    "        print(f\"Keypoints: {keypoints}\")\n",
    "        print(f\"Label: {label}\")\n",
    "\n",
    "print_sample(train_dataset, \"Train Dataset\")\n",
    "print_sample(val_dataset, \"Validation Dataset\")\n",
    "print_sample(test_dataset, \"Test Dataset\")\n",
    "print('\\n')\n",
    "print_sample(train_dataset_seq, \"Train Dataset Sequential\")\n",
    "print_sample(val_dataset_seq, \"Validation Dataset Sequential\")\n",
    "print_sample(test_dataset_seq, \"Test Dataset Sequential\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
