{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\arnav\\\\Documents\\\\University\\\\CS 5100 Foundations of Artificial Intelligence\\\\Final Project\\\\Final Project')\n",
    "\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "from models.tennis_brnn import TennisPoseEstimationModel, TennisPoseSPP\n",
    "from models.tennisnet import TennisNet\n",
    "from training.config import Config\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        json_files = os.listdir(\"../og_dataset/annotations\")\n",
    "        base_path = \"../og_dataset\"\n",
    "        self.train_loader, self.val_loader, self.test_loader = get_dataloaders(json_files, base_path, batch_size=Config.BATCH_SIZE,sequence_length=Config.SEQ_LENGTH)\n",
    "        \n",
    "        self.writer = SummaryWriter(Config.LOG_DIR)\n",
    "        self.model = TennisNet().to(Config.get_device())\n",
    "        self.optimizer = self._initialize_optimizer()\n",
    "        self.scheduler = self._initialize_scheduler(self.train_loader)\n",
    "        self.criterion = self._initialize_criterion()\n",
    "        self.start_epoch, self.best_val_loss = self._load_checkpoint()\n",
    "        self.patience = Config.EARLY_STOPPING_PATIENCE\n",
    "        self.counter = 0\n",
    "\n",
    "    def _initialize_optimizer(self):\n",
    "        \"\"\"\n",
    "        Initializes and returns the optimizer for the model based on the configuration.\n",
    "        The optimizer is selected from a predefined set of optimizers ('adam', 'sgd', 'adamw')\n",
    "        using the configuration specified in the Config class.\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The initialized optimizer based on the configuration.\n",
    "        Raises:\n",
    "            KeyError: If the optimizer specified in Config.OPTIMIZER is not found in the predefined set.\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizers = {\n",
    "            'adam': optim.Adam(self.model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY),\n",
    "            'sgd': optim.SGD(self.model.parameters(), lr=Config.LEARNING_RATE, momentum=Config.MOMENTUM, weight_decay=Config.WEIGHT_DECAY),\n",
    "            'adamw': optim.AdamW(self.model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
    "        }\n",
    "        return optimizers.get(Config.OPTIMIZER)\n",
    "\n",
    "    def _initialize_scheduler(self,train_loader):\n",
    "        \"\"\"\n",
    "        Initializes the learning rate scheduler based on the configuration.\n",
    "        This method sets up different types of learning rate schedulers \n",
    "        such as ReduceLROnPlateau, StepLR, OneCycleLR, and CosineAnnealingLR \n",
    "        based on the specified configuration in the Config class.\n",
    "        Parameters:\n",
    "        - train_loader (DataLoader): The DataLoader for the training dataset, \n",
    "            used to determine the number of steps per epoch for the OneCycleLR scheduler.\n",
    "        Returns:\n",
    "        - torch.optim.lr_scheduler: The initialized learning rate scheduler \n",
    "            as specified in the Config.LR_SCHEDULER.\n",
    "        \"\"\"\n",
    "        \n",
    "        schedulers = {\n",
    "            'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, \n",
    "                mode='min', \n",
    "                factor=Config.LR_SCHEDULER_FACTOR, \n",
    "                patience=Config.LR_SCHEDULER_PATIENCE, \n",
    "                threshold=100\n",
    "                ),\n",
    "            'StepLR': torch.optim.lr_scheduler.StepLR(\n",
    "                self.optimizer, \n",
    "                step_size=Config.LR_STEP_SIZE, \n",
    "                gamma=Config.LR_GAMMA),\n",
    "            'OneCycleLR': torch.optim.lr_scheduler.OneCycleLR(\n",
    "                self.optimizer, \n",
    "                max_lr=Config.LEARNING_RATE * 10, \n",
    "                total_steps=len(train_loader) * Config.EPOCHS,\n",
    "                epochs=Config.EPOCHS, \n",
    "                steps_per_epoch=len(train_loader),  # Add this line to specify the number of batches per epoch\n",
    "                pct_start=0.1, \n",
    "                div_factor=10\n",
    "            ),\n",
    "            'CosineAnnealingLR': torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer, \n",
    "                T_max=Config.EPOCHS//2,\n",
    "                eta_min=Config.LR_MIN\n",
    "                ),\n",
    "        }\n",
    "        return schedulers.get(Config.LR_SCHEDULER)\n",
    "\n",
    "    def _initialize_criterion(self):\n",
    "        \"\"\"\n",
    "        Initializes and returns a dictionary of loss functions for different tasks.\n",
    "        The dictionary contains the following key-value pairs:\n",
    "        - 'keypoints': Mean Squared Error Loss (MSELoss) for keypoint regression.\n",
    "        - 'bbox': Mean Squared Error Loss (MSELoss) for bounding box regression.\n",
    "        - 'classification': Cross Entropy Loss (CrossEntropyLoss) for classification tasks.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys 'keypoints', 'bbox', and 'classification', \n",
    "                  each mapped to their respective loss functions.\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            'keypoints': torch.nn.MSELoss(),\n",
    "            'bbox': torch.nn.MSELoss(),\n",
    "            'classification': torch.nn.CrossEntropyLoss()\n",
    "        }\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_file=f'{Config.CHECKPOINT_DIR}/best_model.pth.tar'):\n",
    "        \"\"\"\n",
    "        Loads a model checkpoint from a specified file.\n",
    "        Args:\n",
    "            checkpoint_file (str): Path to the checkpoint file. Defaults to \n",
    "                                   '{Config.CHECKPOINT_DIR}/best_model.pth.tar'.\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - start_epoch (int): The epoch to start training from.\n",
    "                - best_val_loss (float): The best validation loss recorded.\n",
    "        If the checkpoint file exists, the model and optimizer states are loaded \n",
    "        from the checkpoint, and the start epoch and best validation loss are \n",
    "        returned. If the checkpoint file does not exist, a message is printed and \n",
    "        the function returns 0 and infinity for the start epoch and best validation \n",
    "        loss, respectively.\n",
    "        \"\"\"\n",
    "        \n",
    "        if os.path.isfile(checkpoint_file):\n",
    "            checkpoint = torch.load(checkpoint_file,weights_only=True)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_val_loss = checkpoint['best_val_loss']\n",
    "            print(f\"=> Loaded checkpoint '{checkpoint_file}' (epoch {start_epoch})\")\n",
    "            return start_epoch, best_val_loss\n",
    "        else:\n",
    "            print(f\"=> No checkpoint found at '{checkpoint_file}'\")\n",
    "            return 0, float('inf')\n",
    "        \n",
    "    def _move_to_device(self, images, bboxes, keypoints, labels):\n",
    "        \"\"\"\n",
    "        Moves the given tensors to the specified device.\n",
    "        Args:\n",
    "            images (torch.Tensor): The tensor containing image data.\n",
    "            bboxes (torch.Tensor): The tensor containing bounding box data.\n",
    "            keypoints (torch.Tensor): The tensor containing keypoint data.\n",
    "            labels (torch.Tensor): The tensor containing label data.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the tensors moved to the specified device.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (images.to(Config.get_device()), \n",
    "                bboxes.to(Config.get_device()), \n",
    "                keypoints.to(Config.get_device()), \n",
    "                labels.to(Config.get_device()))\n",
    "\n",
    "    def _check_early_stopping(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Best validation loss: {:.4f} vs Current validation loss: {:.4f}\".format(self.best_val_loss, val_loss))\n",
    "                print(f\"Early stopping at epoch {self.start_epoch + 1}\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def print_gpu_memory():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated, {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the training data loader and evaluates it on the validation data loader.\n",
    "        This method performs the following steps:\n",
    "        1. Sets the model to training mode.\n",
    "        2. Iterates over the epochs specified in the configuration.\n",
    "        3. For each epoch:\n",
    "            a. Initializes the running loss.\n",
    "            b. Prints the current epoch and learning rate.\n",
    "            c. Iterates over the training data batches:\n",
    "                i. Moves the data to the appropriate device (CPU/GPU).\n",
    "                ii. Computes the loss for the current batch.\n",
    "                iii. Accumulates the running loss.\n",
    "                iv. Frees up memory by deleting variables and clearing the CUDA cache.\n",
    "            d. Computes the average loss for the epoch.\n",
    "            e. Logs the training loss to TensorBoard.\n",
    "            f. Evaluates the model on the validation data loader and logs the validation loss.\n",
    "            g. Updates the learning rate scheduler based on the validation loss.\n",
    "            h. Saves the model checkpoint.\n",
    "            i. Checks for early stopping and breaks the loop if triggered.\n",
    "        4. Closes the TensorBoard writer.\n",
    "        5. Prints a message indicating the completion of training.\n",
    "        6. Evaluates the model on the test set.\n",
    "        Note:\n",
    "            - This method assumes that the model, optimizer, scheduler, data loaders, and other necessary components\n",
    "              are already initialized and available as attributes of the class instance.\n",
    "            - The method also assumes that the configuration parameters (e.g., number of epochs, learning rate scheduler type)\n",
    "              are available in a Config class or similar structure.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Starting training...!\\n\")\n",
    "        print(f\"Using {torch.cuda.get_device_name()}\")\n",
    "        print('-'*70)\n",
    "\n",
    "        for epoch in range(self.start_epoch, Config.EPOCHS):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            print(f\"Epoch [{epoch + 1}/{Config.EPOCHS}],\")\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                print(f\"Learning Rate: {param_group['lr']}\")\n",
    "\n",
    "            for images, bboxes, keypoints, labels in tqdm(self.train_loader, desc='Loading training batches'):\n",
    "                images, bboxes, keypoints, labels = self._move_to_device(images, bboxes, keypoints, labels)\n",
    "                print(\"Expected memory usage: \", images.element_size() * images.nelement() / 1e9)\n",
    "                loss = self._train_step(images, bboxes, keypoints, labels)\n",
    "                running_loss += loss.item()\n",
    "                # Delete variables to free up memory\n",
    "                del images, bboxes, keypoints, labels, loss\n",
    "                \n",
    "                # Clear CUDA cache and run garbage collector\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            epoch_loss = running_loss / len(self.train_loader)\n",
    "            self.writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "            print(f'Epoch [{epoch + 1}/{Config.EPOCHS}] Loss: {epoch_loss:.4f}')   \n",
    "\n",
    "            val_loss = self.evaluate(data_loader=self.val_loader, mode='Validation')\n",
    "            self.writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "            print(f'Validation Loss after epoch [{epoch + 1}]: {val_loss:.4f}')\n",
    "            \n",
    "            if Config.LR_SCHEDULER != 'OneCycleLR': \n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "            # self.scheduler.step(val_loss)\n",
    "            self._save_checkpoint(epoch, val_loss)\n",
    "\n",
    "            if self._check_early_stopping(val_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        self.writer.close()\n",
    "        print(\"Training completed.\")\n",
    "        self.evaluate_test_set()\n",
    "\n",
    "    def _train_step(self, images, bboxes, keypoints, labels):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "        Args:\n",
    "            images (torch.Tensor): Batch of input images.\n",
    "            bboxes (torch.Tensor): Ground truth bounding boxes corresponding to the images.\n",
    "            keypoints (torch.Tensor): Ground truth keypoints corresponding to the images.\n",
    "            labels (torch.Tensor): Ground truth labels for classification.\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the training step.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            pred_keypoints, pred_bboxes, classification_logits = self.model(images)\n",
    "                        \n",
    "            keypoints = keypoints[:, -1, :]  # Flatten across all axes except batch\n",
    "            bboxes = bboxes[:, -1, :]        # Flatten across all axes except batch\n",
    "            \n",
    "            assert keypoints.shape == pred_keypoints.shape, f\"Shape mismatch: {keypoints.shape} vs {pred_keypoints.shape}\"\n",
    "            assert bboxes.shape == pred_bboxes.shape, f\"Shape mismatch: {bboxes.shape} vs {pred_bboxes.shape}\"\n",
    "\n",
    "            loss_keypoints = self.criterion['keypoints'](pred_keypoints, keypoints) * Config.LOSS_WEIGHTS['keypoints']\n",
    "            loss_bbox = self.criterion['bbox'](pred_bboxes, bboxes) * Config.LOSS_WEIGHTS['bbox']\n",
    "            loss_classification = self.criterion['classification'](classification_logits, labels) * Config.LOSS_WEIGHTS['classification']\n",
    "            \n",
    "            loss = (loss_keypoints + loss_bbox + loss_classification)\n",
    "            \n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), Config.GRAD_CLIP)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if Config.LR_SCHEDULER == 'OneCycleLR': \n",
    "            self.scheduler.step()\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, data_loader, mode='Validation', max_batches=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data loader.\n",
    "        Args:\n",
    "            data_loader (DataLoader): The data loader containing the dataset to evaluate.\n",
    "            mode (str, optional): The mode of evaluation, either 'Validation' or 'Test'. Defaults to 'Validation'.\n",
    "            max_batches (int, optional): The maximum number of batches to evaluate. If None, evaluate all batches. Defaults to None.\n",
    "        Returns:\n",
    "            float: The average loss over the evaluated batches.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, bboxes, keypoints, labels) in enumerate(tqdm(data_loader, desc=f'Loading {mode} batches')):\n",
    "                if max_batches is not None and i >= max_batches:\n",
    "                    break\n",
    "                images, bboxes, keypoints, labels = self._move_to_device(images, bboxes, keypoints, labels)\n",
    "                \n",
    "                with autocast(device_type='cuda'):\n",
    "                    pred_keypoints, pred_bboxes, classification_logits = self.model(images)\n",
    "                    \n",
    "                    keypoints = keypoints[:, -1, :]\n",
    "                    bboxes = bboxes[:, -1, :]\n",
    "                    \n",
    "                    loss_keypoints = self.criterion['keypoints'](pred_keypoints, keypoints) * Config.LOSS_WEIGHTS['keypoints']\n",
    "                    loss_bbox = self.criterion['bbox'](pred_bboxes, bboxes) * Config.LOSS_WEIGHTS['bbox']\n",
    "                    loss_classification = self.criterion['classification'](classification_logits, labels) * Config.LOSS_WEIGHTS['classification']\n",
    "        \n",
    "                    loss = loss_keypoints + loss_bbox + loss_classification\n",
    "                    running_loss += loss.item()\n",
    "        \n",
    "        return running_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate_test_set(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset using the best saved model checkpoint.\n",
    "        This method performs the following steps:\n",
    "        1. Loads the best model checkpoint from the specified directory.\n",
    "        2. Loads the model state from the checkpoint.\n",
    "        3. Evaluates the model on the test dataset.\n",
    "        4. Prints the test loss.\n",
    "        5. Logs the test loss to TensorBoard.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load the best model\n",
    "        best_model_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth.tar')\n",
    "        if os.path.isfile(best_model_path):\n",
    "            checkpoint = torch.load(best_model_path, map_location=Config.get_device())\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded best model from {best_model_path}\")\n",
    "\n",
    "        test_loss = self.evaluate(self.test_loader, 'Testing')\n",
    "        print(f'Test Loss: {test_loss:.4f}')\n",
    "        self.writer.add_scalar('Loss/test', test_loss)\n",
    "        \n",
    "    def _save_checkpoint(self, epoch, val_loss):\n",
    "        is_best = val_loss < self.best_val_loss\n",
    "        self.best_val_loss = min(val_loss, self.best_val_loss)\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        checkpoint_dir = Config.CHECKPOINT_DIR\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        filepath = os.path.join(checkpoint_dir, 'checkpoint.pth.tar')\n",
    "        torch.save(checkpoint, filepath)\n",
    "        if is_best:\n",
    "            best_filepath = os.path.join(checkpoint_dir, 'best_model.pth.tar')\n",
    "            torch.save(checkpoint, best_filepath)\n",
    "\n",
    "    def _move_to_device(self, images, bboxes, keypoints, labels):\n",
    "        \"\"\"\n",
    "        Moves the given tensors to the specified device.\n",
    "        Args:\n",
    "            images (torch.Tensor): The tensor containing image data.\n",
    "            bboxes (torch.Tensor): The tensor containing bounding box data.\n",
    "            keypoints (torch.Tensor): The tensor containing keypoint data.\n",
    "            labels (torch.Tensor): The tensor containing label data.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the tensors moved to the specified device.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (images.to(Config.get_device()), \n",
    "                bboxes.to(Config.get_device()), \n",
    "                keypoints.to(Config.get_device()), \n",
    "                labels.to(Config.get_device()))\n",
    "\n",
    "    def _check_early_stopping(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Best validation loss: {:.4f} vs Current validation loss: {:.4f}\".format(self.best_val_loss, val_loss))\n",
    "                print(f\"Early stopping at epoch {self.start_epoch + 1}\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the training data loader and evaluates it on the validation data loader.\n",
    "        This method performs the following steps:\n",
    "        1. Sets the model to training mode.\n",
    "        2. Iterates over the epochs specified in the configuration.\n",
    "        3. For each epoch:\n",
    "            a. Initializes the running loss.\n",
    "            b. Prints the current epoch and learning rate.\n",
    "            c. Iterates over the training data batches:\n",
    "                i. Moves the data to the appropriate device (CPU/GPU).\n",
    "                ii. Computes the loss for the current batch.\n",
    "                iii. Accumulates the running loss.\n",
    "                iv. Frees up memory by deleting variables and clearing the CUDA cache.\n",
    "            d. Computes the average loss for the epoch.\n",
    "            e. Logs the training loss to TensorBoard.\n",
    "            f. Evaluates the model on the validation data loader and logs the validation loss.\n",
    "            g. Updates the learning rate scheduler based on the validation loss.\n",
    "            h. Saves the model checkpoint.\n",
    "            i. Checks for early stopping and breaks the loop if triggered.\n",
    "        4. Closes the TensorBoard writer.\n",
    "        5. Prints a message indicating the completion of training.\n",
    "        6. Evaluates the model on the test set.\n",
    "        Note:\n",
    "            - This method assumes that the model, optimizer, scheduler, data loaders, and other necessary components\n",
    "              are already initialized and available as attributes of the class instance.\n",
    "            - The method also assumes that the configuration parameters (e.g., number of epochs, learning rate scheduler type)\n",
    "              are available in a Config class or similar structure.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Starting training...!\\n\")\n",
    "        print(f\"Using {torch.cuda.get_device_name()}\")\n",
    "        print('-'*70)\n",
    "\n",
    "        for epoch in range(self.start_epoch, Config.EPOCHS):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            print(f\"Epoch [{epoch + 1}/{Config.EPOCHS}],\")\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                print(f\"Learning Rate: {param_group['lr']}\")\n",
    "\n",
    "            for images, bboxes, keypoints, labels in tqdm(self.train_loader, desc='Loading training batches'):\n",
    "                images, bboxes, keypoints, labels = self._move_to_device(images, bboxes, keypoints, labels)\n",
    "                # print(\"Expected memory usage: \", images.element_size() * images.nelement() / 1e9)\n",
    "                # print(\"Loaded training batch\")\n",
    "                loss = self._train_step(images, bboxes, keypoints, labels)\n",
    "                running_loss += loss.item()\n",
    "                # Delete variables to free up memory\n",
    "                del images, bboxes, keypoints, labels, loss\n",
    "                \n",
    "                # Clear CUDA cache and run garbage collector\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            epoch_loss = running_loss / len(self.train_loader)\n",
    "            self.writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "            print(f'Epoch [{epoch + 1}/{Config.EPOCHS}] Loss: {epoch_loss:.4f}')   \n",
    "\n",
    "            val_loss = self.evaluate(data_loader=self.val_loader, mode='Validation')\n",
    "            self.writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "            print(f'Validation Loss after epoch [{epoch + 1}]: {val_loss:.4f}')\n",
    "            \n",
    "            if Config.LR_SCHEDULER != 'OneCycleLR': \n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "            # self.scheduler.step(val_loss)\n",
    "            self._save_checkpoint(epoch, val_loss)\n",
    "\n",
    "            if self._check_early_stopping(val_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        self.writer.close()\n",
    "        print(\"Training completed.\")\n",
    "        self.evaluate_test_set()\n",
    "\n",
    "    def _train_step(self, images, bboxes, keypoints, labels):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "        Args:\n",
    "            images (torch.Tensor): Batch of input images.\n",
    "            bboxes (torch.Tensor): Ground truth bounding boxes corresponding to the images.\n",
    "            keypoints (torch.Tensor): Ground truth keypoints corresponding to the images.\n",
    "            labels (torch.Tensor): Ground truth labels for classification.\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the training step.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            pred_keypoints, pred_bboxes, classification_logits = self.model(images)\n",
    "                        \n",
    "            keypoints = keypoints[:, -1, :]  # Flatten across all axes except batch\n",
    "            bboxes = bboxes[:, -1, :]        # Flatten across all axes except batch\n",
    "            \n",
    "            assert keypoints.shape == pred_keypoints.shape, f\"Shape mismatch: {keypoints.shape} vs {pred_keypoints.shape}\"\n",
    "            assert bboxes.shape == pred_bboxes.shape, f\"Shape mismatch: {bboxes.shape} vs {pred_bboxes.shape}\"\n",
    "\n",
    "            loss_keypoints = self.criterion['keypoints'](pred_keypoints, keypoints) * Config.LOSS_WEIGHTS['keypoints']\n",
    "            loss_bbox = self.criterion['bbox'](pred_bboxes, bboxes) * Config.LOSS_WEIGHTS['bbox']\n",
    "            loss_classification = self.criterion['classification'](classification_logits, labels) * Config.LOSS_WEIGHTS['classification']\n",
    "            \n",
    "            loss = (loss_keypoints + loss_bbox + loss_classification)\n",
    "            \n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), Config.GRAD_CLIP)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if Config.LR_SCHEDULER == 'OneCycleLR': \n",
    "            self.scheduler.step()\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, data_loader, mode='Validation', max_batches=None):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data loader.\n",
    "        Args:\n",
    "            data_loader (DataLoader): The data loader containing the dataset to evaluate.\n",
    "            mode (str, optional): The mode of evaluation, either 'Validation' or 'Test'. Defaults to 'Validation'.\n",
    "            max_batches (int, optional): The maximum number of batches to evaluate. If None, evaluate all batches. Defaults to None.\n",
    "        Returns:\n",
    "            float: The average loss over the evaluated batches.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (images, bboxes, keypoints, labels) in enumerate(tqdm(data_loader, desc=f'Loading {mode} batches')):\n",
    "                if max_batches is not None and i >= max_batches:\n",
    "                    break\n",
    "                images, bboxes, keypoints, labels = self._move_to_device(images, bboxes, keypoints, labels)\n",
    "                \n",
    "                with autocast(device_type='cuda'):\n",
    "                    pred_keypoints, pred_bboxes, classification_logits = self.model(images)\n",
    "                    \n",
    "                    keypoints = keypoints[:, -1, :]\n",
    "                    bboxes = bboxes[:, -1, :]\n",
    "                    \n",
    "                    loss_keypoints = self.criterion['keypoints'](pred_keypoints, keypoints) * Config.LOSS_WEIGHTS['keypoints']\n",
    "                    loss_bbox = self.criterion['bbox'](pred_bboxes, bboxes) * Config.LOSS_WEIGHTS['bbox']\n",
    "                    loss_classification = self.criterion['classification'](classification_logits, labels) * Config.LOSS_WEIGHTS['classification']\n",
    "        \n",
    "                    loss = loss_keypoints + loss_bbox + loss_classification\n",
    "                    running_loss += loss.item()\n",
    "        \n",
    "        return running_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate_test_set(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset using the best saved model checkpoint.\n",
    "        This method performs the following steps:\n",
    "        1. Loads the best model checkpoint from the specified directory.\n",
    "        2. Loads the model state from the checkpoint.\n",
    "        3. Evaluates the model on the test dataset.\n",
    "        4. Prints the test loss.\n",
    "        5. Logs the test loss to TensorBoard.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load the best model\n",
    "        best_model_path = os.path.join(Config.CHECKPOINT_DIR, 'best_model.pth.tar')\n",
    "        if os.path.isfile(best_model_path):\n",
    "            checkpoint = torch.load(best_model_path, map_location=Config.get_device())\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded best model from {best_model_path}\")\n",
    "\n",
    "        test_loss = self.evaluate(self.test_loader, 'Testing')\n",
    "        print(f'Test Loss: {test_loss:.4f}')\n",
    "        self.writer.add_scalar('Loss/test', test_loss)\n",
    "        \n",
    "    def _save_checkpoint(self, epoch, val_loss):\n",
    "        is_best = val_loss < self.best_val_loss\n",
    "        self.best_val_loss = min(val_loss, self.best_val_loss)\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        checkpoint_dir = Config.CHECKPOINT_DIR\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        filepath = os.path.join(checkpoint_dir, 'checkpoint.pth.tar')\n",
    "        torch.save(checkpoint, filepath)\n",
    "        if is_best:\n",
    "            best_filepath = os.path.join(checkpoint_dir, 'best_model.pth.tar')\n",
    "            torch.save(checkpoint, best_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlaps found in data\n",
      "Freezing the backbone layers of efficientnet_b3\n",
      "=> No checkpoint found at 'checkpoints_net/best_model.pth.tar'\n",
      "GPU Memory: 0.21 GB allocated, 0.44 GB reserved, 0.44 GB cached, Total: 8.59 GB\n",
      "Running a quick test run to check if everything works...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick training:   2%|▏         | 1/44 [00:20<14:53, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run training batch 1 loss: 50002.4883\n",
      "GPU Memory: 0.45 GB allocated, 4.48 GB reserved\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick training:   5%|▍         | 2/44 [00:21<06:08,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run training batch 2 loss: 49254.1914\n",
      "GPU Memory: 0.45 GB allocated, 4.48 GB reserved\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quick training:   5%|▍         | 2/44 [00:28<09:52, 14.11s/it]\n",
      "Loading Validation batches:  20%|██        | 2/10 [00:24<01:39, 12.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run validation loss: 9579.9859\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Testing batches:  22%|██▏       | 2/9 [00:21<01:14, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run test loss: 11136.9570\n",
      "Quick test run completed!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated, {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved, {torch.cuda.memory_reserved() / 1e9:.2f} GB cached, Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "# Quick test run\n",
    "print(\"Running a quick test run to check if everything works...\")\n",
    "\n",
    "print('-'*80)\n",
    "# Quick training step\n",
    "for i, (images, bboxes, keypoints, labels) in enumerate(tqdm(trainer.train_loader, desc='Quick training')):\n",
    "    if i >= 2:  # Run only for 2 batches\n",
    "        break\n",
    "    images, bboxes, keypoints, labels = trainer._move_to_device(images, bboxes, keypoints, labels)\n",
    "    loss = trainer._train_step(images, bboxes, keypoints, labels)\n",
    "    print(f\"Test run training batch {i + 1} loss: {loss.item():.4f}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated, {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")\n",
    "    print('-'*80)\n",
    "\n",
    "\n",
    "# Quick validation step\n",
    "val_loss = trainer.evaluate(data_loader=trainer.val_loader, mode='Validation', max_batches=2)\n",
    "print(f\"Test run validation loss: {val_loss:.4f}\")\n",
    "print('-'*80)\n",
    "\n",
    "# Quick testing step\n",
    "test_loss = trainer.evaluate(data_loader=trainer.test_loader, mode='Testing', max_batches=2)\n",
    "print(f\"Test run test loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"Quick test run completed!\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# trainer.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAI_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
